from typing import Iterable

from langchain_community.document_loaders import DirectoryLoader
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter,
)
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings


def add_to_metadata(docs: Iterable[Document]) -> Iterable[Document]:
    """ìƒí’ˆ íŒë§¤ì •ë³´ (ê°€ê²©, ìƒí’ˆëª… ë“±)ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ `DocumentLoader`ë¡œ ë¡œë“œëœ docì˜ ë©”íƒ€ë°ì´í„°ë¡œ ë“±ë¡"""
    import re

    pattern = r"- (\w+): (.+)"
    mapping = {
        "prdDisplayName": "ìƒí’ˆëª…",
        "prdBrand": "ë¸Œëœë“œ",
        "prdPrice": "ì •ê°€",
        "prdSalePrice": "íŒë§¤ê°€",
        "prdCategory": "ë¶„ë¥˜",
    }

    for doc in docs:
        text_prd = re.search(
            r"(?<=## íŒë§¤ ì •ë³´).+(?=## ìƒí’ˆì •ë³´ ì œê³µê³ ì‹œ)", doc.page_content, re.DOTALL
        ).group()  # re.DOTALL setting for matching '.' with newlines as well
        matches = re.findall(pattern, text_prd)
        new_metadata = {}
        for match in matches:
            if match[0] in ["prdPrice", "prdSalePrice"]:
                try:
                    new_metadata[mapping[match[0]]] = int(
                        match[1].strip("ì›,'~ ").replace(",", "")
                    )
                except:
                    new_metadata["ì •ê°€"] = None
            else:
                new_metadata[mapping[match[0]]] = match[1].strip(",' ")
        if new_metadata["ì •ê°€"] is None:  #  no sale
            new_metadata["ì •ê°€"] = new_metadata["íŒë§¤ê°€"]
        doc.metadata.update(new_metadata)
    return docs


## loading and chunking
loader = DirectoryLoader(
    "backend/database/raw/0128/",
    glob="**/*.md.txt",
    loader_cls=TextLoader,
)
docs = loader.load()
docs = add_to_metadata(docs)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=5000,  # gpt-4-0125-preview: 128,000 tokens
    chunk_overlap=150,
    separators=["#{2,5}", "\n---+\n+", "\n\n+"],
    keep_separator=True,
    is_separator_regex=True,
    add_start_index=True,
)

chunks = splitter.split_documents(docs)
# print(chunks)

## vectorization
vs = Chroma.from_documents(
    documents=chunks,
    embedding=OpenAIEmbeddings(),
    persist_directory="backend/database/vectorstore/chroma",
    ids=[str(i) for i in range(1, len(chunks) + 1)],
    collection_name="test-0129",
    collection_metadata={
        "hnsw:space": "cosine",
    },
)
print(f"ğŸ’¥New collection made w/ {vs._collection.count()} document(s).")

## to use
# persistent_client = chromadb.PersistentClient()
# collection = persistent_client.get_or_create_collection("collection_name")
